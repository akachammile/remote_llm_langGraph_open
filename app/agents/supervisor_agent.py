import re
import os
import json
import base64
import traceback
from app.agents import *
from app.logger import logger
from pydantic import Field
from app.database.utils import KnowledgeFile
from app.graphs.graph_state import AgentState
from typing import List, Optional, Dict, Union, Set
from app.schemas.schema import Message, ToolChoice
from langgraph.graph import StateGraph, START, END
from langgraph.graph.state import CompiledStateGraph
from app.prompts.supervisor_prompt import SYSTEM_PROMPT, TOOL_PROMPT, USER_PROMPT
from app.tools.tool_collection import ToolCollection
from app.tools.planning_tool import PlanningTool

from app.tools.file_process_tool import FileProcessTool
from app.tools.image_segmentation_tool import ImageSegmentationTool



class SupervisorAgent(BaseAgent):
    name: str = "SupervisorAgent"
    description: str = "ç”¨äºç®¡ç†å’Œåè°ƒå¤šä¸ªå­Agentçš„å·¥ä½œ"
    system_prompt: str = SYSTEM_PROMPT
    tool_prompt: str = TOOL_PROMPT
    user_prompt: str = USER_PROMPT
    current_step: int = 1
    available_tools: ToolCollection = Field(
        default_factory=lambda: ToolCollection(
            PlanningTool(),ImageSegmentationTool(), FileProcessTool(), 
        )
    )

    def __init__(self):
        super().__init__()
        self._graph: Optional[CompiledStateGraph] = None
        self.agent_infos: List[Dict[str, str]] = self.get_all_agent_info()
        self.placehold_prompt: str = self._build_prompt()
        
        if not self.placehold_prompt:
            logger.warning("SupervisorAgent åˆå§‹åŒ–è­¦å‘Šï¼šplacehold_prompt ä¸ºç©ºï¼Œä½¿ç”¨ fallback_prompt")
            self.placehold_prompt = ""

        self.chat_history: str = ""

    @staticmethod
    def get_all_agent_info() -> List[Dict[str, str]]:
        """è·å–æ‰€æœ‰å­Agentä¸­çš„nameä»¥åŠdescriptionä¿¡æ¯"""

        # TODO æ­¤å¤„éœ€è¦ä¼˜åŒ–ï¼Œç³»ç»ŸAgentçš„æè¿°åªéœ€è¦æ›´æ–°ä¸€æ¬¡ï¼Œé™¤éæœ‰æ”¹åŠ¨ï¼Œ è¿˜æ²¡ä¿®æ”¹
        def all_subclasses(cls):
            subclasses = set(cls.__subclasses__())
            for subclass in cls.__subclasses__():
                subclasses.update(all_subclasses(subclass))
            return subclasses

        agents_description = []
        for agent_cls in all_subclasses(BaseAgent):
            # å¢åŠ æ‰€æœ‰Agentåç§°ã€åŠŸèƒ½åŠå…¶å·¥å…·æè¿°
            if agent_cls.model_fields["name"].default != "SupervisorAgent":
                name = (
                    agent_cls.model_fields["name"].default
                    if "name" in agent_cls.model_fields
                    else agent_cls.__name__
                )
                description = (
                    agent_cls.model_fields["description"].default
                    if "description" in agent_cls.model_fields
                    else ""
                )
                tool = (
                    agent_cls.model_fields["tool"].default
                    if "tool" in agent_cls.model_fields
                    else ""
                )
                
                agents_description.append(
                    {"name": name, "description": description, "tool": tool}
                ) 
        return agents_description

    @staticmethod
    def get_agents() -> List[str]:
        """è·å–æ‰€æœ‰å­Agentçš„nameåˆ—è¡¨"""
        agent_infos = SupervisorAgent.get_all_agent_info()
        return [a["name"] for a in agent_infos]

    def _build_prompt(self) -> str:
        """å°è£…system_promptå’Œç”¨æˆ·é—®é¢˜ä¸ºChatPromptTemplateå¹¶æ ¼å¼åŒ–ä¸ºmessages"""
        try:
            tool_list = "\n".join(
                [
                    # f'{agent["name"]}: {agent["description"]}'
                    str(
                        {
                            "name": agent["name"],
                            "description": agent["description"],
                            "tool": agent["tool"],
                        }
                    )
                    for agent in self.agent_infos
                    if agent.get("name") != "SupervisorAgent"
                ]
            )
            prompt_template: str = self.tool_prompt.format(
                tool_list=tool_list,
            )
            return prompt_template
        except Exception as e:
            logger.error(f"å½“å‰build_prompt_erroré”™è¯¯ä¸º: {str(traceback.format_exc())}")

    async def chat_response(
        self, message: str, file_list: List[Union[str]]
    ) -> AgentState:
        # FIXME æ­¤å¤„éœ€è¦ä¼˜åŒ–æ”¯æŒç±»å‹
        # result = cut_query(text=message)

        IMAGE_EXTENSIONS: Set[str] = {"png", "jpg", "jpeg", "bmp", "tif"}
        encoded_string = ""
        extension = ""
        image_path = ""
        # åˆ›å»ºå›¾
        if self._graph is None:
            self._graph = await self.create_supervisor_graph()

        # æ›´æ–°è®°å¿†åº“
        # if message:
        #     self.update_agent_memory(role="user", content=message)
        #     logger.info(f"æ€»ä½“è®°å¿†:{self.memory.messages}")

        try:
            if file_list:
                for file_path in file_list:
                    image_path, extension = os.path.splitext(
                        file_path
                    )  # åœ¨å¾ªç¯å†…è·å–æ‰©å±•å
                    extension = extension.lower().lstrip(".")
                    if extension in IMAGE_EXTENSIONS:
                        with open(file_path, "rb") as image_file:
                            encoded_string = base64.b64encode(image_file.read()).decode("utf-8")

            state: AgentState = {
                "question": message,
                "memory": self.chat_history,
                "image_data": encoded_string,
                "image_format": extension,
                "memory": self.memory.get_recent_messages(10),
                "image_path": image_path,
            }

            response: AgentState = await self._graph.ainvoke(state)
            return response
        except Exception as e:
            logger.error(f"Error: {e}\n{traceback.format_exc()}")


    def route_next_agent(self, state: dict) -> str:
        # FIXME, éœ€è¦é¢å¤–æ·»åŠ æ¡ä»¶
        """åˆ¤æ–­è·¯ç”±æ¡ä»¶ï¼Œæ ¹æ®èŠ‚ç‚¹è¿”å›çš„å†…å®¹åˆ¤æ–­ä¸‹ä¸€ä¸ªéœ€è¦æ‰§è¡Œçš„èŠ‚ç‚¹

        Args:
            state (dict): _description_

        Returns:
            str: _description_
        """
        return state.get("next_agent", "END")
    
        

    async def top_level_supervisor(self, state: AgentState) -> AgentState:  # type: ignore
        """é¡¶å±‚SupervisorèŠ‚ç‚¹, å†³å®šä¸‹ä¸€ä¸ªå­Agent"""
        # TODO: éœ€è¦æ·»åŠ å¾ªç¯åˆ¤æ–­ï¼Œæ¯”å¦‚å­—èŠ‚ç‚¹ä»»åŠ¡å®Œæˆåï¼Œè‹¥æ˜¯è¿”å›SupervisorèŠ‚ç‚¹æ—¶å€™ï¼Œéœ€è¦å¢åŠ é¢å¤–çš„è¿‡æ»¤æ¡ä»¶ï¼Œä¸å¯ä»¥ä¸€ç›´å¾ªç¯çš„èµ°Supervisor
        # state.setdefault("history", [])
        # state.setdefault("sub_task", [])
        # state["history"].append(state.get("next_agent", "SupervisorAgent"))
        
        # if state.get("sub_task"):
        #     logger.info("ğŸ¤” æ­£åœ¨æ€è€ƒä¸‹ä¸€æ­¥ä»»åŠ¡")
        #     state = await self.next_step(state)
        #     return state
        
        user_message = Message.user_message(content=state["question"], base64_image=state["image_data"])
        system_message = Message.system_message(self.system_prompt)
        logger.info(f"ğŸ¤” å·¥å…·æƒ…å†µ: {self.available_tools.to_params()}")
        response = await self.llm.ask_tool_v2(
            messages=[user_message],
            system_msgs=[system_message],
            tools=self.available_tools.to_params(),
            tool_choice=ToolChoice.AUTO,
        )
        logger.info(f"ğŸ¤” æ€è€ƒç»“æœä¸º: {response}")
        logger.info(f"ğŸ”§ è°ƒç”¨å·¥å…·ä¸º: {response.tool_calls}")
        if response.tool_calls:
            for tool in response.tool_calls:
                tool_name = tool.function.name if tool.function.name else None
                if tool_name == "image_segmentation":
                    segmentation_tool = self.available_tools.get_tool("image_segmentation")
                    params = {
                        "image_data": state["image_data"],
                        "image_path": state["image_path"],
                        "state":state
                        }
                    segmented_bytes, segmentation_info, save_path = segmentation_tool.execute(**params)
                    state["sub_task"].append(save_path)
                    logger.info(f"å›¾ç‰‡åˆ†å‰²ç»“æœä¸º: {save_path}")
                    logger.info(f"state: {state["sub_task"]}")
                elif tool_name == "planning_tool":
                    planning_tool = self.available_tools.get_tool(tool_name)
                    params = {
                        "user_message": user_message,
                        "tools": self.available_tools.to_params()[1:],
                        }
                    _ = await planning_tool.execute(**params)
                    
                    

        
                    
                    
        
        
        # ğŸ§± æ„å»ºè¾“å…¥æ¶ˆæ¯ï¼ˆä»…é¦–æ¬¡æˆ–é‡æ–°è§„åˆ’æ—¶ä½¿ç”¨ï¼‰
        # if not state.get("messages"):  # å¦‚æœæ²¡æœ‰ç°æˆä¸Šä¸‹æ–‡ï¼Œåˆ™ç”¨åŸºç¡€ prompt
        #     user_message = self.user_prompt.format(query=state["question"])
        #     messages = self.system_prompt + self.placehold_prompt + user_message
        # else:
        #     # å¦åˆ™ä¿ç•™å·²æœ‰ä¸Šä¸‹æ–‡ï¼Œè®©æ¨¡å‹æ¥ç€ä¸Šæ¬¡çŠ¶æ€æ€è€ƒ
        #     messages = self.system_prompt + self.placehold_prompt + \
        #         str(state["messages"][-1].content)
        # # response = await self.llm.ask_tool(messages, state)
        # response = await self.llm.ask_tool_v2(messages, state)
        # json_str = response.content
        # match = re.search(r"\{[\s\S]*\}", json_str)
        # if match:
        #     json_str = match.group(0)
        # response = json.loads(json_str)
        # new_state = state.copy()
        
        # if response["task"]["name"]  == "ChatAgent":
        # # å¦‚æœæ˜¯ç®€å•å¯¹è¯ï¼Œç›´æ¥æ›´æ–° messages å¹¶å‡†å¤‡ç»“æŸ
        #     new_state["next_agent"] = "exit"      # ä¸‹ä¸€æ­¥ç›´æ¥ç»“æŸ            new_state["messages"] = description  # å°†å›ç­”æ”¾å…¥ messages
        #     new_state["messages"] = response["response"]  # å°†å›ç­”æ”¾å…¥ messages
        # else:
        #     # å¦‚æœæ˜¯å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸åˆ†å‘
        #     new_state["next_agent"] = response["task"]["name"] 
        #     new_state["messages"] = response["response"]
        # return new_state
    # async def chat_node(self, state: AgentState) -> AgentState:
    #     """ä¸“é—¨å¤„ç†ç®€å•å¯¹è¯çš„èŠ‚ç‚¹ï¼Œç„¶åç›´æ¥ç»“æŸæµç¨‹ã€‚"""
    #     logger.info("ğŸ’¬ æ­£åœ¨å¤„ç†ç®€å•å¯¹è¯ï¼Œæµç¨‹å³å°†ç»“æŸã€‚")
    #     return state
    async def next_step(self, state: AgentState) -> AgentState:
        """LLM-basedåæ€æœºåˆ¶"""
        next_step_prompt = f"""
        ä¿®æ­£è§’è‰²å®šä½,ä½ æ˜¯ä¸€ä¸ªæµç¨‹è®¡åˆ’ä¸“å®¶,ä½ éœ€è¦æ ¹æ®å†å²ä»»åŠ¡ç¡®å®šä¸‹ä¸€ä¸ªä»»åŠ¡ã€‚
        - å¦‚æœç”¨æˆ·é—®é¢˜å·²ç»å¾—åˆ°å®Œæ•´è§£ç­”ï¼Œå­ä»»åŠ¡å‡æ ‡è®°ä¸ºâ€œå·²å®Œæˆâ€ï¼Œä½ å¿…é¡»ç»“æŸä»»åŠ¡ï¼ˆå¦‚ç»“æŸä»»åŠ¡exitï¼‰
        æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç»“åˆæ¨¡æ¿ä¸­çš„æ–‡ä»¶,è§„åˆ’ä¸‹ä¸€æ­¥ï¼ˆå¦‚ç»“æŸä»»åŠ¡ENDæˆ–é€‰æ‹©å…¶ä»–å·¥å…·ç»§ç»­æ‰§è¡Œï¼‰ã€‚
        è¾“å‡ºè§„åˆ’è¯·ä¸¥æ ¼éµå¾ªè¾“å‡ºè¦æ±‚
        ç”¨æˆ·é—®é¢˜ä¸­çš„ä»»åŠ¡æ˜¯: {state['question']}
        å­ä»»åŠ¡çš„å®Œæˆæƒ…å†µä¸ºï¼š{state['sub_task']}
        """
        reflect_prompt = self.system_prompt + next_step_prompt + self.placehold_prompt 
        response = await self.llm.ask_tool(reflect_prompt, state)
        match = re.search(r"\{[\s\S]*\}", response.content)
        if match:
            plan = json.loads(match.group(0))
            state["next_agent"] = plan["task"]["name"]
            state["messages"] = plan["response"]
            state["status"] = "replanned"
        else: 
            state["status"] = "failed"
        return state
    
    
    
    async def reflect_and_replan(self, state: AgentState) -> AgentState:
        """LLM-basedåæ€æœºåˆ¶"""
        reflect_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åæ€ä¸“å®¶,ä½ éœ€è¦æ ¹æ®å†å²ä»»åŠ¡ä»¥åŠæä¾›çš„ç›¸å…³ä¿¡æ¯å›ç­”æ˜¯ç¬¦åˆå½“å‰é—®é¢˜ã€‚
        ç”¨æˆ·é—®é¢˜æ˜¯: {state['question']}
        å­ä»»åŠ¡çš„ç»“æœä¸ºï¼š{state['messages'][-1].content}

        æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç»“åˆæ¨¡æ¿ä¸­çš„æ–‡ä»¶,è§„åˆ’ä¸‹ä¸€æ­¥ï¼ˆå¦‚è¾“å‡ºENDä»¥æç¤ºç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼‰ã€‚
        è¾“å‡ºè§„åˆ’è¯·ä¸¥æ ¼éµå¾ªè¾“å‡ºè¦æ±‚
        """
        print(f"æç¤ºå‡ºç°çš„é—®é¢˜æ˜¯ï¼š{state['messages'][-1].content}")
        
        reflect_prompt = self.system_prompt + reflect_prompt + self.placehold_prompt 
        response = await self.llm.ask_tool(reflect_prompt, state)
        match = re.search(r"\{[\s\S]*\}", response.content)
        if match:
            plan = json.loads(match.group(0))
            state["next_agent"] = plan["task"]["name"]
            state["messages"] = plan["response"]
            state["status"] = "replanned"
            state["reflection"] = False
        else:
            state["status"] = "failed"
        return state
    async def create_supervisor_graph(self):
        """æ„å»ºSupervisorçš„çŠ¶æ€å›¾"""
        if self._graph is None:
            try:
                supervisor_builder = StateGraph(AgentState)
                           
                vision_subgraph = VisionAgent().build_subgraph()
                doc_subgraph = DocAgent().build_subgraph()

                # æ·»åŠ èŠ‚ç‚¹
                supervisor_builder.add_node("top_level_supervisor", self.top_level_supervisor)
                supervisor_builder.add_node("VisionAgent", vision_subgraph)
                # supervisor_builder.add_node("ChatAgent", self.chat_node)
                supervisor_builder.add_node("DocAgent", doc_subgraph)
                # supervisor_builder.add_node("reflection_node", self.reflect_and_replan)
                # supervisor_builder.add_node("planning_node", self.next_step)

                # æ·»åŠ è¾¹
                supervisor_builder.add_edge(START, "top_level_supervisor")
                supervisor_builder.add_conditional_edges(
                    "top_level_supervisor",
                    self.route_next_agent,
                    {
                        "VisionAgent": "VisionAgent",
                        # "ChatAgent": "ChatAgent",
                        "DocAgent": "DocAgent",
                        # "reflection_node": "reflection_node",
                        # "planning_node": "planning_node",
                        "exit": END,
                    },
                )
                supervisor_builder.add_edge("VisionAgent", "top_level_supervisor")
                # supervisor_builder.add_edge("ChatAgent", "top_level_supervisor")
                supervisor_builder.add_edge("DocAgent", "top_level_supervisor")
                self._graph = supervisor_builder.compile()
                logger.info("SupervisorçŠ¶æ€å›¾åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error("Graphåˆ›å»ºå¤±è´¥", error=str(e))

                raise e
        return self._graph
